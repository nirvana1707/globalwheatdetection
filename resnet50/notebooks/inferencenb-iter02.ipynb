{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "from PIL import Image\n",
    "\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision import transforms\n",
    "\n",
    "import torch.utils.data\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "\n",
    "import albumentations\n",
    "from albumentations.pytorch.transforms import ToTensorV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_instance_objectdetection_model(num_classes,path_weight):\n",
    "    # load an instance segmentation model pre-trained on COCO\n",
    "    create_model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=False,pretrained_backbone=False)\n",
    "\n",
    "    # get the number of input features for the classifier\n",
    "    in_features = create_model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace the pre-trained head with a new one\n",
    "    create_model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    create_model.load_state_dict(torch.load(path_weight,map_location=torch.device('cpu')))\n",
    "\n",
    "    return create_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.listdir(\"/kaggle/input/gwd-thirditeration-weights/customtrained_fasterrcnn_resnet50_fpn_augementation_28052020_1340.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path_weight = \"/kaggle/input/global_wheat_detection/fasterrcnn_resnet50_fpn_best.pth\"\n",
    "#path_weight = \"/kaggle/input/gwd-augmentation-training-weights-ver01/customtrained_fasterrcnn_resnet50_fpn_augementation (1).pth\"\n",
    "path_weight = \"/kaggle/input/gwd-thirditeration-weights/customtrained_fasterrcnn_resnet50_fpn_augementation_28052020_1340.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "# Why 2 classes - background and wheat-heads\n",
    "trained_model = get_instance_objectdetection_model(num_classes,path_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "def get_test_transform():\n",
    "    return albumentations.Compose([\n",
    "        # A.Resize(512, 512),\n",
    "        ToTensorV2(p=1.0)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlobalWheatDetectionTestDataset(torch.utils.data.Dataset):\n",
    "    # first lets start with __init__ and initialize any objects\n",
    "    def __init__(self,input_df,input_dir,transforms=None):\n",
    "        \n",
    "        self.df=input_df\n",
    "        \n",
    "        self.list_images = list(self.df['image_id'].unique())\n",
    "        \n",
    "        self.image_dir=input_dir\n",
    "        \n",
    "        self.transforms = transforms\n",
    "    \n",
    "    # next lets define __getitem__\n",
    "    # very important to note what it returns for EACH image:\n",
    "    # I. image - a PIL image of size (H,W) for ResNet50 FPN image should be scaled\n",
    "    \n",
    "    # II. image_id \n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        \n",
    "        # II. image_id\n",
    "        img_id = self.list_images[idx]\n",
    "        # I. Input image\n",
    "        # Specifications: A.RGB format B. scaled (0,1) C. size (H,W) D. PIL format\n",
    "        \n",
    "        img = cv2.imread(self.image_dir+\"/\"+img_id+\".jpg\")\n",
    "        img_RGB = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
    "        img_scaled = img_RGB/255.0\n",
    "        img_final = img_scaled\n",
    "        ret_image = {}\n",
    "        ret_image['image']=img_final\n",
    "        if self.transforms is not None:\n",
    "            res = self.transforms(**ret_image)\n",
    "            img_final = res['image']\n",
    "            #img_final = torch.tensor(img_final, dtype=torch.float32)\n",
    "        \n",
    "        return img_final, img_id\n",
    "    \n",
    "    # next lets define __len__    \n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.df['image_id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test=pd.read_csv(\"/kaggle/input/global-wheat-detection/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir=\"/kaggle/input/global-wheat-detection/test/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>PredictionString</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aac893a91</td>\n",
       "      <td>1.0 0 0 50 50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>51f1be19e</td>\n",
       "      <td>1.0 0 0 50 50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>f5a1f0358</td>\n",
       "      <td>1.0 0 0 50 50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>796707dd7</td>\n",
       "      <td>1.0 0 0 50 50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>51b3e36ab</td>\n",
       "      <td>1.0 0 0 50 50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    image_id PredictionString\n",
       "0  aac893a91    1.0 0 0 50 50\n",
       "1  51f1be19e    1.0 0 0 50 50\n",
       "2  f5a1f0358    1.0 0 0 50 50\n",
       "3  796707dd7    1.0 0 0 50 50\n",
       "4  51b3e36ab    1.0 0 0 50 50"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = GlobalWheatDetectionTestDataset(df_test,test_dir,get_test_transform())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(test_dataset, batch_size=8,shuffle=False, num_workers=1,collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_threshold = 0.45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_prediction_string(boxes, scores): ## Define the formate for storing prediction results\n",
    "    pred_strings = []\n",
    "    for j in zip(scores, boxes):\n",
    "        pred_strings.append(\"{0:.4f} {1} {2} {3} {4}\".format(j[0], j[1][0], j[1][1], j[1][2], j[1][3]))\n",
    "\n",
    "    return \" \".join(pred_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, img_ids = next(iter(test_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/opt/conda/conda-bld/pytorch_1587428398394/work/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of nonzero is deprecated:\n",
      "\tnonzero(Tensor input, *, Tensor out)\n",
      "Consider using one of the following signatures instead:\n",
      "\tnonzero(Tensor input, *, bool as_tuple)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>PredictionString</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aac893a91</td>\n",
       "      <td>0.9980 557 533 129 184 0.9946 68 0 100 159 0.9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>51f1be19e</td>\n",
       "      <td>0.9979 606 79 162 177 0.9959 839 261 136 204 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>f5a1f0358</td>\n",
       "      <td>0.9968 136 747 166 123 0.9967 938 426 85 192 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>796707dd7</td>\n",
       "      <td>0.9947 375 630 103 118 0.9945 940 67 83 104 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>51b3e36ab</td>\n",
       "      <td>0.9986 833 448 187 147 0.9982 541 27 250 132 0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    image_id                                   PredictionString\n",
       "0  aac893a91  0.9980 557 533 129 184 0.9946 68 0 100 159 0.9...\n",
       "1  51f1be19e  0.9979 606 79 162 177 0.9959 839 261 136 204 0...\n",
       "2  f5a1f0358  0.9968 136 747 166 123 0.9967 938 426 85 192 0...\n",
       "3  796707dd7  0.9947 375 630 103 118 0.9945 940 67 83 104 0....\n",
       "4  51b3e36ab  0.9986 833 448 187 147 0.9982 541 27 250 132 0..."
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Lets make the prediction\n",
    "results=[]\n",
    "trained_model.to(device)\n",
    "trained_model.eval()\n",
    "images = []\n",
    "outputs =[]\n",
    "for images_, image_ids in test_dataloader:    \n",
    "\n",
    "    images = list(image.to(device,dtype=torch.float) for image in images_)\n",
    "    outputs = trained_model(images)\n",
    "\n",
    "    for i, image in enumerate(images):\n",
    "\n",
    "        boxes = outputs[i]['boxes'].data.cpu().numpy()    ##Formate of the output's box is [Xmin,Ymin,Xmax,Ymax]\n",
    "        scores = outputs[i]['scores'].data.cpu().numpy()\n",
    "        \n",
    "        boxes = boxes[scores >= detection_threshold].astype(np.int32) #Compare the score of output with the threshold and\n",
    "        scores = scores[scores >= detection_threshold]                    #slelect only those boxes whose score is greater\n",
    "                                                                          # than threshold value\n",
    "        image_id = image_ids[i]\n",
    "        \n",
    "        boxes[:, 2] = boxes[:, 2] - boxes[:, 0]         \n",
    "        boxes[:, 3] = boxes[:, 3] - boxes[:, 1]         #Convert the box formate to [Xmin,Ymin,W,H]\n",
    "        \n",
    "        \n",
    "            \n",
    "        result = {                                     #Store the image id and boxes and scores in result dict.\n",
    "            'image_id': image_id,\n",
    "            'PredictionString': format_prediction_string(boxes, scores)\n",
    "        }\n",
    "\n",
    "        \n",
    "        results.append(result)              #Append the result dict to Results list\n",
    "\n",
    "test_df = pd.DataFrame(results, columns=['image_id', 'PredictionString'])\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
